{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f953cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danny\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651438d9",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210121fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1021976 points, Test: 253393 points\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Preprocessing\n",
    "for df in [train_df, test_df]:\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'], format='mixed')\n",
    "    df['total_seconds'] = df['time_stamp'].astype('int64')\n",
    "    \n",
    "train_df = train_df.sort_values(['traj_ind', 'total_seconds'])\n",
    "test_df = test_df.sort_values(['traj_ind', 'total_seconds'])\n",
    "\n",
    "# Add kinematics\n",
    "def add_kinematics(df):\n",
    "    dt = df.groupby('traj_ind')['time_stamp'].diff().dt.total_seconds()\n",
    "    dx = df.groupby('traj_ind')['x'].diff()\n",
    "    dy = df.groupby('traj_ind')['y'].diff()\n",
    "    dz = df.groupby('traj_ind')['z'].diff()\n",
    "    \n",
    "    df['v_x'] = dx / dt\n",
    "    df['v_y'] = dy / dt\n",
    "    df['v_z'] = dz / dt\n",
    "    df['speed'] = (df['v_x']**2 + df['v_y']**2 + df['v_z']**2)**0.5\n",
    "    df['step_dist'] = (dx**2 + dy**2 + dz**2)**0.5\n",
    "    return df\n",
    "\n",
    "train_df = add_kinematics(train_df)\n",
    "test_df = add_kinematics(test_df)\n",
    "print(f\"Train: {len(train_df)} points, Test: {len(test_df)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b76dbe",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (Same as RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8641d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting scalar features...\n",
      "Extracting scalar features...\n",
      "Extracting physics features...\n",
      "Extracting physics features...\n",
      "Features shape: (32741, 58)\n",
      "Class distribution: {1: 7940, 0: 22462, 2: 2339}\n"
     ]
    }
   ],
   "source": [
    "def extract_trajectory_features(df):\n",
    "    \"\"\"Extract scalar aggregated features per trajectory.\"\"\"\n",
    "    print(\"Extracting scalar features...\")\n",
    "    grp = df.groupby('traj_ind')\n",
    "    \n",
    "    aggs = grp.agg({\n",
    "        'x': ['first', 'last', 'min', 'max', 'mean', 'std'],\n",
    "        'y': ['first', 'last', 'min', 'max', 'mean', 'std'],\n",
    "        'z': ['first', 'last', 'min', 'max', 'mean', 'std'],\n",
    "        'total_seconds': ['first', 'last', 'count'],\n",
    "        'speed': ['first', 'mean', 'max', 'std'], \n",
    "        'v_x': ['first', 'mean', 'std'], \n",
    "        'v_y': ['first', 'mean', 'std'], \n",
    "        'v_z': ['first', 'mean', 'std'], \n",
    "        'step_dist': ['sum']\n",
    "    })\n",
    "    aggs.columns = ['_'.join(col).strip() for col in aggs.columns.values]\n",
    "    \n",
    "    # Derived features\n",
    "    aggs['duration'] = aggs['total_seconds_last'] - aggs['total_seconds_first']\n",
    "    aggs['dist_start_end'] = np.sqrt(\n",
    "        (aggs['x_last'] - aggs['x_first'])**2 + \n",
    "        (aggs['y_last'] - aggs['y_first'])**2 + \n",
    "        (aggs['z_last'] - aggs['z_first'])**2\n",
    "    )\n",
    "    aggs['straightness'] = aggs['dist_start_end'] / aggs['step_dist_sum']\n",
    "    \n",
    "    # Launch angles\n",
    "    vx = aggs['v_x_first'].fillna(0)\n",
    "    vy = aggs['v_y_first'].fillna(0)\n",
    "    vz = aggs['v_z_first'].fillna(0)\n",
    "    aggs['launch_azimuth'] = np.arctan2(vy, vx)\n",
    "    aggs['launch_elevation'] = np.arctan2(vz, np.sqrt(vx**2 + vy**2))\n",
    "    \n",
    "    aggs.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    aggs = aggs.fillna(0)\n",
    "    return aggs\n",
    "\n",
    "def extract_physics_features(df):\n",
    "    \"\"\"Extract parabolic fit coefficients.\"\"\"\n",
    "    print(\"Extracting physics features...\")\n",
    "    features = []\n",
    "    indices = []\n",
    "    \n",
    "    for traj_id, group in df.groupby('traj_ind'):\n",
    "        t_ns = group['total_seconds'].values\n",
    "        t = (t_ns - t_ns[0]) / 1e9\n",
    "        \n",
    "        x, y, z = group['x'].values, group['y'].values, group['z'].values\n",
    "        vx = group['v_x'].fillna(method='bfill').fillna(0).values\n",
    "        vy = group['v_y'].fillna(method='bfill').fillna(0).values\n",
    "        vz = group['v_z'].fillna(method='bfill').fillna(0).values\n",
    "        \n",
    "        row = {}\n",
    "        \n",
    "        # Position parabolic fit\n",
    "        if len(t) >= 3:\n",
    "            try:\n",
    "                for axis, data in zip(['x', 'y', 'z'], [x, y, z]):\n",
    "                    c, res, _, _, _ = np.polyfit(t, data, 2, full=True)\n",
    "                    row[f'c{axis}_2'], row[f'c{axis}_1'], row[f'c{axis}_0'] = c\n",
    "                    row[f'res_{axis}'] = res[0] if len(res) > 0 else 0.0\n",
    "            except:\n",
    "                for axis in ['x', 'y', 'z']:\n",
    "                    row[f'c{axis}_2'] = row[f'c{axis}_1'] = row[f'c{axis}_0'] = row[f'res_{axis}'] = 0\n",
    "        else:\n",
    "            for axis in ['x', 'y', 'z']:\n",
    "                row[f'c{axis}_2'] = row[f'c{axis}_1'] = row[f'c{axis}_0'] = row[f'res_{axis}'] = 0\n",
    "        \n",
    "        # Velocity linear fit (acceleration)\n",
    "        if len(t) >= 2:\n",
    "            try:\n",
    "                t_r = t.reshape(-1, 1)\n",
    "                for axis, data in zip(['x', 'y', 'z'], [vx, vy, vz]):\n",
    "                    m = LinearRegression().fit(t_r, data)\n",
    "                    row[f'acc_{axis}'] = m.coef_[0]\n",
    "                    row[f'v0_fit_{axis}'] = m.intercept_\n",
    "            except:\n",
    "                for axis in ['x', 'y', 'z']:\n",
    "                    row[f'acc_{axis}'] = row[f'v0_fit_{axis}'] = 0\n",
    "        else:\n",
    "            for axis in ['x', 'y', 'z']:\n",
    "                row[f'acc_{axis}'] = row[f'v0_fit_{axis}'] = 0\n",
    "        \n",
    "        features.append(row)\n",
    "        indices.append(traj_id)\n",
    "    \n",
    "    return pd.DataFrame(features, index=indices)\n",
    "\n",
    "# Extract features\n",
    "X_train_scalar = extract_trajectory_features(train_df)\n",
    "X_test_scalar = extract_trajectory_features(test_df)\n",
    "X_train_phys = extract_physics_features(train_df)\n",
    "X_test_phys = extract_physics_features(test_df)\n",
    "\n",
    "# Combine\n",
    "X_train_full = pd.concat([X_train_scalar, X_train_phys], axis=1)\n",
    "X_test_full = pd.concat([X_test_scalar, X_test_phys], axis=1)\n",
    "\n",
    "# Labels\n",
    "y_train = train_df.groupby('traj_ind')['label'].first().loc[X_train_full.index]\n",
    "\n",
    "print(f\"Features shape: {X_train_full.shape}\")\n",
    "print(f\"Class distribution: {dict(Counter(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153b579",
   "metadata": {},
   "source": [
    "## 3. Create Barrage Sequences\n",
    "\n",
    "Group trajectories into barrages (continuous time windows from same location).\n",
    "Each barrage becomes a **sequence** that the Transformer will process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5750512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating barrage sequences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8016 barrage sequences\n",
      "Sequence lengths: min=2, max=6, mean=4.1\n",
      "Creating barrage sequences...\n",
      "Created 1985 barrage sequences\n",
      "Sequence lengths: min=2, max=6, mean=4.1\n",
      "\n",
      "Feature dimension: 56\n"
     ]
    }
   ],
   "source": [
    "def create_barrage_sequences(X_df, y_series=None, time_threshold_min=5):\n",
    "    \"\"\"\n",
    "    Group trajectories into barrage sequences.\n",
    "    Each barrage is a sequence of trajectories from the same location within time_threshold.\n",
    "    \"\"\"\n",
    "    print(\"Creating barrage sequences...\")\n",
    "    df = X_df.copy()\n",
    "    \n",
    "    # Sort by location and time\n",
    "    df = df.sort_values(['x_first', 'y_first', 'z_first', 'total_seconds_first'])\n",
    "    \n",
    "    # Identify barrages\n",
    "    TIME_THRESHOLD_NS = time_threshold_min * 60 * 1_000_000_000\n",
    "    grp_cols = ['x_first', 'y_first', 'z_first']\n",
    "    df['time_diff'] = df.groupby(grp_cols)['total_seconds_first'].diff()\n",
    "    df['new_barrage'] = (df['time_diff'].isna()) | (df['time_diff'] > TIME_THRESHOLD_NS)\n",
    "    df['barrage_id'] = df['new_barrage'].cumsum()\n",
    "    \n",
    "    # Select feature columns (exclude metadata)\n",
    "    feature_cols = [c for c in df.columns if c not in \n",
    "                    ['time_diff', 'new_barrage', 'barrage_id', 'total_seconds_first', 'total_seconds_last']]\n",
    "    \n",
    "    # Group into sequences\n",
    "    sequences = []\n",
    "    labels_seq = []\n",
    "    traj_indices = []\n",
    "    barrage_ids = []\n",
    "    \n",
    "    for barrage_id, group in df.groupby('barrage_id'):\n",
    "        # Sort by time within barrage\n",
    "        group = group.sort_values('total_seconds_first')\n",
    "        \n",
    "        seq_features = group[feature_cols].values.astype(np.float32)\n",
    "        seq_traj_idx = group.index.tolist()\n",
    "        \n",
    "        sequences.append(seq_features)\n",
    "        traj_indices.append(seq_traj_idx)\n",
    "        barrage_ids.append(barrage_id)\n",
    "        \n",
    "        if y_series is not None:\n",
    "            seq_labels = y_series.loc[group.index].values.astype(np.int64)\n",
    "            labels_seq.append(seq_labels)\n",
    "    \n",
    "    print(f\"Created {len(sequences)} barrage sequences\")\n",
    "    print(f\"Sequence lengths: min={min(len(s) for s in sequences)}, \"\n",
    "          f\"max={max(len(s) for s in sequences)}, \"\n",
    "          f\"mean={np.mean([len(s) for s in sequences]):.1f}\")\n",
    "    \n",
    "    if y_series is not None:\n",
    "        return sequences, labels_seq, traj_indices, barrage_ids, feature_cols\n",
    "    return sequences, traj_indices, barrage_ids, feature_cols\n",
    "\n",
    "# Create sequences\n",
    "train_seqs, train_labels, train_traj_idx, train_barrage_ids, feature_cols = \\\n",
    "    create_barrage_sequences(X_train_full, y_train)\n",
    "test_seqs, test_traj_idx, test_barrage_ids, _ = \\\n",
    "    create_barrage_sequences(X_test_full)\n",
    "\n",
    "print(f\"\\nFeature dimension: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8639188",
   "metadata": {},
   "source": [
    "## 4. Transformer Model Architecture\n",
    "\n",
    "The model uses **self-attention** to let each trajectory see all other trajectories in the barrage.\n",
    "This way, each prediction is informed by the \"whole picture\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd335ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model defined.\n",
      "\n",
      "Key insight: Self-attention lets each trajectory 'see' all other\n",
      "trajectories in the barrage, so predictions consider the full context.\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding to preserve sequence order.\"\"\"\n",
    "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BarrageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for barrage-level sequence classification.\n",
    "    \n",
    "    Key features:\n",
    "    - Self-attention allows each trajectory to attend to ALL other trajectories\n",
    "    - Each position gets a classification that considers the full barrage context\n",
    "    - Positional encoding preserves temporal order within the barrage\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=4, \n",
    "                 dim_feedforward=256, num_classes=3, dropout=0.2, max_len=500):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection: features -> transformer dimension\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm for better training stability\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head (per position)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, input_dim) - input features\n",
    "            src_key_padding_mask: (batch, seq_len) - True for padding positions\n",
    "            \n",
    "        Returns:\n",
    "            logits: (batch, seq_len, num_classes) - per-position predictions\n",
    "        \"\"\"\n",
    "        # Project input\n",
    "        x = self.input_projection(x)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer encoder (self-attention across all positions)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Classify each position\n",
    "        logits = self.classifier(x)  # (batch, seq_len, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"Transformer model defined.\")\n",
    "print(\"\\nKey insight: Self-attention lets each trajectory 'see' all other\")\n",
    "print(\"trajectories in the barrage, so predictions consider the full context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1f517",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38f94c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and collate function defined.\n"
     ]
    }
   ],
   "source": [
    "class BarrageDataset(Dataset):\n",
    "    def __init__(self, sequences, labels=None, scaler=None, fit_scaler=False):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Normalize features\n",
    "        if fit_scaler:\n",
    "            all_data = np.vstack(sequences)\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(all_data)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "        \n",
    "        # Apply scaling\n",
    "        self.scaled_seqs = []\n",
    "        for seq in sequences:\n",
    "            scaled = self.scaler.transform(seq)\n",
    "            scaled = np.nan_to_num(scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            self.scaled_seqs.append(scaled)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.tensor(self.scaled_seqs[idx], dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return seq, label\n",
    "        return seq\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences and create attention masks.\"\"\"\n",
    "    if isinstance(batch[0], tuple):\n",
    "        seqs, labels = zip(*batch)\n",
    "        lengths = [len(s) for s in seqs]\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "        # Pad sequences\n",
    "        seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        \n",
    "        # Create padding mask (True = padding, False = real data)\n",
    "        padding_mask = torch.zeros(len(seqs), max_len, dtype=torch.bool)\n",
    "        for i, length in enumerate(lengths):\n",
    "            padding_mask[i, length:] = True\n",
    "        \n",
    "        return seqs_padded, labels_padded, padding_mask, torch.tensor(lengths)\n",
    "    else:\n",
    "        seqs = batch\n",
    "        lengths = [len(s) for s in seqs]\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "        seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        padding_mask = torch.zeros(len(seqs), max_len, dtype=torch.bool)\n",
    "        for i, length in enumerate(lengths):\n",
    "            padding_mask[i, length:] = True\n",
    "        \n",
    "        return seqs_padded, padding_mask, torch.tensor(lengths)\n",
    "\n",
    "\n",
    "print(\"Dataset and collate function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66f79c",
   "metadata": {},
   "source": [
    "## 6. Training Setup (Leak-Proof Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c0717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train barrages: 6412\n",
      "Val barrages: 1604\n",
      "\n",
      "Train labels: {0: 17939, 1: 6353, 2: 1862}\n",
      "Val labels: {1: 1587, 0: 4523, 2: 477}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Split by barrage (keep barrages together)\n",
    "n_barrages = len(train_seqs)\n",
    "barrage_indices = list(range(n_barrages))\n",
    "\n",
    "# Get majority label for each barrage (for stratification)\n",
    "barrage_majority_labels = [Counter(labels).most_common(1)[0][0] for labels in train_labels]\n",
    "\n",
    "# Simple train/val split by barrage\n",
    "train_bar_idx, val_bar_idx = train_test_split(\n",
    "    barrage_indices, test_size=0.2, random_state=42,\n",
    "    stratify=barrage_majority_labels\n",
    ")\n",
    "\n",
    "# Split sequences\n",
    "train_seqs_split = [train_seqs[i] for i in train_bar_idx]\n",
    "train_labels_split = [train_labels[i] for i in train_bar_idx]\n",
    "val_seqs_split = [train_seqs[i] for i in val_bar_idx]\n",
    "val_labels_split = [train_labels[i] for i in val_bar_idx]\n",
    "\n",
    "print(f\"Train barrages: {len(train_seqs_split)}\")\n",
    "print(f\"Val barrages: {len(val_seqs_split)}\")\n",
    "\n",
    "# Count labels\n",
    "train_label_counts = Counter(np.concatenate(train_labels_split))\n",
    "val_label_counts = Counter(np.concatenate(val_labels_split))\n",
    "print(f\"\\nTrain labels: {dict(train_label_counts)}\")\n",
    "print(f\"Val labels: {dict(val_label_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60845e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.2229, 0.6294, 2.1476], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = BarrageDataset(train_seqs_split, train_labels_split, fit_scaler=True)\n",
    "val_dataset = BarrageDataset(val_seqs_split, val_labels_split, scaler=train_dataset.scaler)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "all_train_labels = np.concatenate(train_labels_split)\n",
    "class_counts = np.bincount(all_train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ecb6206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 56])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[11][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251bc50",
   "metadata": {},
   "source": [
    "## 7. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6dab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 545,923\n",
      "Input dimension: 56\n",
      "Model dimension: 128\n",
      "Attention heads: 8\n",
      "Transformer layers: 4\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "input_dim = len(feature_cols)\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "dim_feedforward = 256\n",
    "dropout = 0.2\n",
    "\n",
    "model = BarrageTransformer(\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    num_classes=3,\n",
    "    dropout=dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Attention heads: {nhead}\")\n",
    "print(f\"Transformer layers: {num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c963bc7",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf600ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for seqs, labels, padding_mask, lengths in loader:\n",
    "        seqs = seqs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        padding_mask = padding_mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(seqs, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        logits_flat = logits.view(-1, 3)\n",
    "        labels_flat = labels.view(-1)\n",
    "        loss = criterion(logits_flat, labels_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seqs, labels, padding_mask, lengths in loader:\n",
    "            seqs = seqs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            \n",
    "            logits = model(seqs, src_key_padding_mask=padding_mask)\n",
    "            \n",
    "            logits_flat = logits.view(-1, 3)\n",
    "            labels_flat = labels.view(-1)\n",
    "            loss = criterion(logits_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Collect valid predictions\n",
    "            for i in range(len(lengths)):\n",
    "                seq_len = lengths[i].item()\n",
    "                all_preds.extend(preds[i, :seq_len].cpu().numpy())\n",
    "                all_labels.extend(labels[i, :seq_len].cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d944e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer model...\n",
      "================================================================================\n",
      "Epoch   1 | Train Loss: 0.3402 | Val Loss: 0.1849 | Recalls: [0.944, 0.882, 0.981] | Min: 0.882\n",
      "Epoch   6 | Train Loss: 0.2220 | Val Loss: 0.1709 | Recalls: [0.952, 0.903, 0.990] | Min: 0.903\n",
      "Epoch  11 | Train Loss: 0.2322 | Val Loss: 0.1519 | Recalls: [0.953, 0.911, 0.977] | Min: 0.911\n",
      "Epoch  16 | Train Loss: 0.1762 | Val Loss: 0.1569 | Recalls: [0.963, 0.917, 0.971] | Min: 0.917\n",
      "Epoch  21 | Train Loss: 0.1530 | Val Loss: 0.1585 | Recalls: [0.950, 0.937, 0.975] | Min: 0.937\n",
      "Epoch  26 | Train Loss: 0.1448 | Val Loss: 0.1528 | Recalls: [0.957, 0.933, 0.973] | Min: 0.933\n",
      "\n",
      "Early stopping at epoch 29\n",
      "\n",
      "Loaded best model with min recall: 0.9452\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_min_recall = 0\n",
    "best_model_state = None\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training Transformer model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Calculate recalls\n",
    "    recalls = recall_score(val_labels, val_preds, average=None, labels=[0, 1, 2])\n",
    "    min_recall = min(recalls)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Recalls: [{recalls[0]:.3f}, {recalls[1]:.3f}, {recalls[2]:.3f}] | Min: {min_recall:.3f}\")\n",
    "    \n",
    "    # Save best model (by min recall)\n",
    "    if min_recall > best_min_recall:\n",
    "        best_min_recall = min_recall\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "print(f\"\\nLoaded best model with min recall: {best_min_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582fb54",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d523ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSFORMER MODEL - FINAL VALIDATION RESULTS\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Label 0       0.99      0.95      0.97      4523\n",
      "     Label 1       0.85      0.95      0.90      1587\n",
      "     Label 2       0.96      0.95      0.96       477\n",
      "\n",
      "    accuracy                           0.95      6587\n",
      "   macro avg       0.93      0.95      0.94      6587\n",
      "weighted avg       0.95      0.95      0.95      6587\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "        Pred 0  Pred 1  Pred 2\n",
      "True 0    4275     245       3\n",
      "True 1      58    1513      16\n",
      "True 2       0      23     454\n",
      "\n",
      "Per-class Recalls: Label 0=0.9452, Label 1=0.9534, Label 2=0.9518\n",
      "Minimum Recall: 0.9452\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSFORMER MODEL - FINAL VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(val_labels, val_preds, target_names=['Label 0', 'Label 1', 'Label 2']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "print(pd.DataFrame(cm, \n",
    "                   index=['True 0', 'True 1', 'True 2'],\n",
    "                   columns=['Pred 0', 'Pred 1', 'Pred 2']))\n",
    "\n",
    "final_recalls = recall_score(val_labels, val_preds, average=None, labels=[0, 1, 2])\n",
    "print(f\"\\nPer-class Recalls: Label 0={final_recalls[0]:.4f}, Label 1={final_recalls[1]:.4f}, Label 2={final_recalls[2]:.4f}\")\n",
    "print(f\"Minimum Recall: {min(final_recalls):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec7b5a",
   "metadata": {},
   "source": [
    "## 10. Visualize Attention (Optional)\n",
    "\n",
    "This shows how much each trajectory \"attends to\" other trajectories in the barrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c48016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing barrage 1 with 6 trajectories\n",
      "Labels in this barrage: [1 1 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_attention(model, dataset, sample_idx=0):\n",
    "    \"\"\"Visualize attention weights for a sample barrage.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get sample\n",
    "    if isinstance(dataset[sample_idx], tuple):\n",
    "        seq, labels = dataset[sample_idx]\n",
    "    else:\n",
    "        seq = dataset[sample_idx]\n",
    "        labels = None\n",
    "    \n",
    "    seq = seq.unsqueeze(0).to(device)  # (1, seq_len, features)\n",
    "    seq_len = seq.shape[1]\n",
    "    \n",
    "    # Get attention weights from first layer\n",
    "    with torch.no_grad():\n",
    "        # Project input\n",
    "        x = model.input_projection(seq)\n",
    "        x = model.pos_encoder(x)\n",
    "        \n",
    "        # Get attention from first layer\n",
    "        layer = model.transformer_encoder.layers[0]\n",
    "        # Self attention\n",
    "        attn_output, attn_weights = layer.self_attn(\n",
    "            x, x, x, need_weights=True, average_attn_weights=True\n",
    "        )\n",
    "    \n",
    "    attn = attn_weights[0].cpu().numpy()  # (seq_len, seq_len)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xlabel('Key Position (other trajectories)')\n",
    "    ax.set_ylabel('Query Position (current trajectory)')\n",
    "    ax.set_title(f'Attention Weights (Barrage with {seq_len} trajectories)\\n'\n",
    "                 'Each row shows how much one trajectory attends to others')\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    if labels is not None:\n",
    "        labels_np = labels.numpy()\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        ax.set_xticklabels([f'L{l}' for l in labels_np], fontsize=8)\n",
    "        ax.set_yticklabels([f'L{l}' for l in labels_np], fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attn\n",
    "\n",
    "# Find a barrage with multiple trajectories for visualization\n",
    "for i, (seq, labels) in enumerate(val_dataset):\n",
    "    if len(seq) >= 5 and len(seq) <= 20:  # Medium-sized barrage\n",
    "        print(f\"Visualizing barrage {i} with {len(seq)} trajectories\")\n",
    "        print(f\"Labels in this barrage: {labels.numpy()}\")\n",
    "        attn = visualize_attention(model, val_dataset, i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53cfba",
   "metadata": {},
   "source": [
    "## 11. Generate Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94eb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_dataset = BarrageDataset(test_seqs, scaler=train_dataset.scaler)\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    seqs = batch\n",
    "    lengths = [len(s) for s in seqs]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    padding_mask = torch.zeros(len(seqs), max_len, dtype=torch.bool)\n",
    "    for i, length in enumerate(lengths):\n",
    "        padding_mask[i, length:] = True\n",
    "    \n",
    "    return seqs_padded, padding_mask, torch.tensor(lengths)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_test)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, padding_mask, lengths in test_loader:\n",
    "        seqs = seqs.to(device)\n",
    "        padding_mask = padding_mask.to(device)\n",
    "        \n",
    "        logits = model(seqs, src_key_padding_mask=padding_mask)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        for i in range(len(lengths)):\n",
    "            seq_len = lengths[i].item()\n",
    "            all_test_preds.append(preds[i, :seq_len].cpu().numpy())\n",
    "\n",
    "# Map back to trajectory indices\n",
    "pred_flat = []\n",
    "traj_flat = []\n",
    "for i, barrage_preds in enumerate(all_test_preds):\n",
    "    traj_indices = test_traj_idx[i]\n",
    "    for j, pred in enumerate(barrage_preds):\n",
    "        pred_flat.append(pred)\n",
    "        traj_flat.append(traj_indices[j])\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'traj_ind': traj_flat,\n",
    "    'label': pred_flat\n",
    "})\n",
    "submission = submission.sort_values('traj_ind')\n",
    "\n",
    "output_file = 'submission_transformer.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"Submission saved to {output_file}\")\n",
    "print(f\"Predictions: {len(submission)} trajectories\")\n",
    "print(f\"Label distribution: {dict(Counter(submission['label']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0691170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x_first', 'x_last', 'x_min', 'x_max', 'x_mean', 'x_std', 'y_first',\n",
       "       'y_last', 'y_min', 'y_max', 'y_mean', 'y_std', 'z_first', 'z_last',\n",
       "       'z_min', 'z_max', 'z_mean', 'z_std', 'total_seconds_first',\n",
       "       'total_seconds_last', 'total_seconds_count', 'speed_first',\n",
       "       'speed_mean', 'speed_max', 'speed_std', 'v_x_first', 'v_x_mean',\n",
       "       'v_x_std', 'v_y_first', 'v_y_mean', 'v_y_std', 'v_z_first', 'v_z_mean',\n",
       "       'v_z_std', 'step_dist_sum', 'duration', 'dist_start_end',\n",
       "       'straightness', 'launch_azimuth', 'launch_elevation', 'cx_2', 'cx_1',\n",
       "       'cx_0', 'res_x', 'cy_2', 'cy_1', 'cy_0', 'res_y', 'cz_2', 'cz_1',\n",
       "       'cz_0', 'res_z', 'acc_x', 'v0_fit_x', 'acc_y', 'v0_fit_y', 'acc_z',\n",
       "       'v0_fit_z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d468272",
   "metadata": {},
   "source": [
    "## 5-Fold GroupKFold Cross-Validation (Transformer vs RF)\n",
    "\n",
    "Compare both models with fully leak-proof validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12253ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "5-FOLD GROUPKFOLD CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      "======================================== FOLD 1 ========================================\n",
      "Transformer - Recalls: [0.9534, 0.9383, 0.9408] | Min: 0.9383\n",
      "Random Forest - Recalls: [0.9742, 0.8580, 0.8858] | Min: 0.8580\n",
      "\n",
      "======================================== FOLD 2 ========================================\n",
      "Transformer - Recalls: [0.9660, 0.9514, 0.9581] | Min: 0.9514\n",
      "Random Forest - Recalls: [0.9770, 0.8854, 0.8719] | Min: 0.8719\n",
      "\n",
      "======================================== FOLD 3 ========================================\n",
      "Transformer - Recalls: [0.9620, 0.9553, 0.9535] | Min: 0.9535\n",
      "Random Forest - Recalls: [0.9738, 0.8815, 0.8643] | Min: 0.8643\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION SUMMARY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import gc\n",
    "\n",
    "# Prepare data for CV\n",
    "n_folds = 3\n",
    "gkf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "# Get barrage IDs for each trajectory (for RF)\n",
    "train_with_barrage = X_train_full.copy()\n",
    "train_with_barrage = train_with_barrage.sort_values(['x_first', 'y_first', 'z_first', 'total_seconds_first'])\n",
    "TIME_THRESHOLD_NS = 5 * 60 * 1_000_000_000\n",
    "grp_cols = ['x_first', 'y_first', 'z_first']\n",
    "train_with_barrage['time_diff'] = train_with_barrage.groupby(grp_cols)['total_seconds_first'].diff()\n",
    "train_with_barrage['new_barrage'] = (train_with_barrage['time_diff'].isna()) | (train_with_barrage['time_diff'] > TIME_THRESHOLD_NS)\n",
    "train_with_barrage['barrage_id'] = train_with_barrage['new_barrage'].cumsum()\n",
    "traj_to_barrage = train_with_barrage['barrage_id']\n",
    "\n",
    "# Feature columns for RF\n",
    "rf_feature_cols = [c for c in X_train_full.columns if c not in ['total_seconds_first', 'total_seconds_last']]\n",
    "\n",
    "# Results storage\n",
    "transformer_fold_results = []\n",
    "rf_fold_results = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"5-FOLD GROUPKFOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get unique barrages and their majority labels\n",
    "unique_barrages = list(range(len(train_seqs)))\n",
    "barrage_groups = np.array(unique_barrages)\n",
    "\n",
    "for fold, (train_bar_idx, val_bar_idx) in enumerate(gkf.split(unique_barrages, barrage_majority_labels, barrage_groups)):\n",
    "    print(f\"\\n{'='*40} FOLD {fold+1} {'='*40}\")\n",
    "    \n",
    "    # ===================== TRANSFORMER =====================\n",
    "    # Split sequences\n",
    "    train_seqs_fold = [train_seqs[i] for i in train_bar_idx]\n",
    "    train_labels_fold = [train_labels[i] for i in train_bar_idx]\n",
    "    val_seqs_fold = [train_seqs[i] for i in val_bar_idx]\n",
    "    val_labels_fold = [train_labels[i] for i in val_bar_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset_fold = BarrageDataset(train_seqs_fold, train_labels_fold, fit_scaler=True)\n",
    "    val_dataset_fold = BarrageDataset(val_seqs_fold, val_labels_fold, scaler=train_dataset_fold.scaler)\n",
    "    \n",
    "    train_loader_fold = DataLoader(train_dataset_fold, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader_fold = DataLoader(val_dataset_fold, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Class weights\n",
    "    all_labels_fold = np.concatenate(train_labels_fold)\n",
    "    counts = np.bincount(all_labels_fold)\n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model_fold = BarrageTransformer(\n",
    "        input_dim=input_dim, d_model=d_model, nhead=nhead,\n",
    "        num_layers=num_layers, dim_feedforward=dim_feedforward,\n",
    "        num_classes=3, dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion_fold = nn.CrossEntropyLoss(weight=weights_tensor, ignore_index=-100)\n",
    "    optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Train\n",
    "    best_min_recall_fold = 0\n",
    "    best_state_fold = None\n",
    "    patience_counter_fold = 0\n",
    "    \n",
    "    for epoch in range(30):  # Fewer epochs for CV\n",
    "        model_fold.train()\n",
    "        for seqs, labels, padding_mask, lengths in train_loader_fold:\n",
    "            seqs, labels, padding_mask = seqs.to(device), labels.to(device), padding_mask.to(device)\n",
    "            optimizer_fold.zero_grad()\n",
    "            logits = model_fold(seqs, src_key_padding_mask=padding_mask)\n",
    "            loss = criterion_fold(logits.view(-1, 3), labels.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_fold.parameters(), 1.0)\n",
    "            optimizer_fold.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model_fold.eval()\n",
    "        all_preds, all_labels_eval = [], []\n",
    "        with torch.no_grad():\n",
    "            for seqs, labels, padding_mask, lengths in val_loader_fold:\n",
    "                seqs, padding_mask = seqs.to(device), padding_mask.to(device)\n",
    "                logits = model_fold(seqs, src_key_padding_mask=padding_mask)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                for i in range(len(lengths)):\n",
    "                    seq_len = lengths[i].item()\n",
    "                    all_preds.extend(preds[i, :seq_len].cpu().numpy())\n",
    "                    all_labels_eval.extend(labels[i, :seq_len].numpy())\n",
    "        \n",
    "        recalls = recall_score(all_labels_eval, all_preds, average=None, labels=[0, 1, 2])\n",
    "        min_rec = min(recalls)\n",
    "        \n",
    "        if min_rec > best_min_recall_fold:\n",
    "            best_min_recall_fold = min_rec\n",
    "            best_state_fold = {k: v.cpu().clone() for k, v in model_fold.state_dict().items()}\n",
    "            patience_counter_fold = 0\n",
    "        else:\n",
    "            patience_counter_fold += 1\n",
    "            if patience_counter_fold >= 7:\n",
    "                break\n",
    "    \n",
    "    # Load best and evaluate\n",
    "    model_fold.load_state_dict({k: v.to(device) for k, v in best_state_fold.items()})\n",
    "    model_fold.eval()\n",
    "    all_preds, all_labels_eval = [], []\n",
    "    with torch.no_grad():\n",
    "        for seqs, labels, padding_mask, lengths in val_loader_fold:\n",
    "            seqs, padding_mask = seqs.to(device), padding_mask.to(device)\n",
    "            logits = model_fold(seqs, src_key_padding_mask=padding_mask)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            for i in range(len(lengths)):\n",
    "                seq_len = lengths[i].item()\n",
    "                all_preds.extend(preds[i, :seq_len].cpu().numpy())\n",
    "                all_labels_eval.extend(labels[i, :seq_len].numpy())\n",
    "    \n",
    "    tf_recalls = recall_score(all_labels_eval, all_preds, average=None, labels=[0, 1, 2])\n",
    "    transformer_fold_results.append(tf_recalls)\n",
    "    print(f\"Transformer - Recalls: [{tf_recalls[0]:.4f}, {tf_recalls[1]:.4f}, {tf_recalls[2]:.4f}] | Min: {min(tf_recalls):.4f}\")\n",
    "    \n",
    "    # ===================== RANDOM FOREST =====================\n",
    "    # Get trajectory indices for this fold\n",
    "    train_barrage_set = set([train_barrage_ids[i] for i in train_bar_idx])\n",
    "    val_barrage_set = set([train_barrage_ids[i] for i in val_bar_idx])\n",
    "    \n",
    "    train_traj_mask = traj_to_barrage.isin(train_barrage_set)\n",
    "    val_traj_mask = traj_to_barrage.isin(val_barrage_set)\n",
    "    \n",
    "    X_train_rf = X_train_full.loc[train_traj_mask, rf_feature_cols]\n",
    "    y_train_rf = y_train.loc[train_traj_mask]\n",
    "    X_val_rf = X_train_full.loc[val_traj_mask, rf_feature_cols]\n",
    "    y_val_rf = y_train.loc[val_traj_mask]\n",
    "    \n",
    "    # Train RF\n",
    "    rf_fold = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=20, min_samples_split=5,\n",
    "        min_samples_leaf=2, class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_fold.fit(X_train_rf, y_train_rf)\n",
    "    y_pred_rf = rf_fold.predict(X_val_rf)\n",
    "    \n",
    "    rf_recalls = recall_score(y_val_rf, y_pred_rf, average=None, labels=[0, 1, 2])\n",
    "    rf_fold_results.append(rf_recalls)\n",
    "    print(f\"Random Forest - Recalls: [{rf_recalls[0]:.4f}, {rf_recalls[1]:.4f}, {rf_recalls[2]:.4f}] | Min: {min(rf_recalls):.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model_fold, train_dataset_fold, val_dataset_fold, rf_fold\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "121c5901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON: TRANSFORMER vs RANDOM FOREST (5-Fold GroupKFold CV)\n",
      "================================================================================\n",
      "\n",
      "--- Per-Fold Min Recall ---\n",
      "Fold     Transformer     Random Forest   Winner         \n",
      "-----------------------------------------------------\n",
      "Fold 1   0.9383          0.8580          Transformer\n",
      "Fold 2   0.9514          0.8719          Transformer\n",
      "Fold 3   0.9535          0.8643          Transformer\n",
      "-----------------------------------------------------\n",
      "Mean     0.9478          0.8647         \n",
      "Std      0.0067          0.0057         \n",
      "Min      0.9383          0.8580         \n",
      "Max      0.9535          0.8719         \n",
      "\n",
      "--- Per-Class Recall (Mean  Std) ---\n",
      "Model           Label 0              Label 1              Label 2             \n",
      "---------------------------------------------------------------------------\n",
      "Transformer     0.9605  0.0053   0.9484  0.0073   0.9508  0.0073\n",
      "Random Forest   0.9750  0.0014   0.8750  0.0121   0.8740  0.0089\n",
      "\n",
      "================================================================================\n",
      " WINNER: TRANSFORMER (by 8.30 percentage points)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Aggregate results\n",
    "transformer_results = np.array(transformer_fold_results)\n",
    "rf_results = np.array(rf_fold_results)\n",
    "\n",
    "# Calculate statistics\n",
    "tf_min_recalls = transformer_results.min(axis=1)\n",
    "rf_min_recalls = rf_results.min(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL COMPARISON: TRANSFORMER vs RANDOM FOREST (5-Fold GroupKFold CV)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Per-Fold Min Recall ---\")\n",
    "print(f\"{'Fold':<8} {'Transformer':<15} {'Random Forest':<15} {'Winner':<15}\")\n",
    "print(\"-\" * 53)\n",
    "for i in range(n_folds):\n",
    "    tf_min = tf_min_recalls[i]\n",
    "    rf_min = rf_min_recalls[i]\n",
    "    winner = \"Transformer\" if tf_min > rf_min else \"RF\"\n",
    "    print(f\"Fold {i+1:<3} {tf_min:<15.4f} {rf_min:<15.4f} {winner}\")\n",
    "\n",
    "print(\"-\" * 53)\n",
    "print(f\"{'Mean':<8} {tf_min_recalls.mean():<15.4f} {rf_min_recalls.mean():<15.4f}\")\n",
    "print(f\"{'Std':<8} {tf_min_recalls.std():<15.4f} {rf_min_recalls.std():<15.4f}\")\n",
    "print(f\"{'Min':<8} {tf_min_recalls.min():<15.4f} {rf_min_recalls.min():<15.4f}\")\n",
    "print(f\"{'Max':<8} {tf_min_recalls.max():<15.4f} {rf_min_recalls.max():<15.4f}\")\n",
    "\n",
    "print(\"\\n--- Per-Class Recall (Mean  Std) ---\")\n",
    "print(f\"{'Model':<15} {'Label 0':<20} {'Label 1':<20} {'Label 2':<20}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Transformer':<15} {transformer_results[:,0].mean():.4f}  {transformer_results[:,0].std():.4f}   \"\n",
    "      f\"{transformer_results[:,1].mean():.4f}  {transformer_results[:,1].std():.4f}   \"\n",
    "      f\"{transformer_results[:,2].mean():.4f}  {transformer_results[:,2].std():.4f}\")\n",
    "print(f\"{'Random Forest':<15} {rf_results[:,0].mean():.4f}  {rf_results[:,0].std():.4f}   \"\n",
    "      f\"{rf_results[:,1].mean():.4f}  {rf_results[:,1].std():.4f}   \"\n",
    "      f\"{rf_results[:,2].mean():.4f}  {rf_results[:,2].std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "overall_winner = \"TRANSFORMER\" if tf_min_recalls.mean() > rf_min_recalls.mean() else \"RANDOM FOREST\"\n",
    "diff = abs(tf_min_recalls.mean() - rf_min_recalls.mean()) * 100\n",
    "print(f\" WINNER: {overall_winner} (by {diff:.2f} percentage points)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd2620",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### How the Transformer Uses Context\n",
    "\n",
    "```\n",
    "Barrage (time window):  [Traj_1, Traj_2, Traj_3, Traj_4, Traj_5]\n",
    "                                                        \n",
    "                            \n",
    "                                    Self-Attention\n",
    "                            \n",
    "                                                        \n",
    "                                                        \n",
    "Predictions:            [Pred_1, Pred_2, Pred_3, Pred_4, Pred_5]\n",
    "```\n",
    "\n",
    "**Each prediction considers:**\n",
    "1. Its own trajectory features\n",
    "2. All other trajectories in the same barrage (via attention)\n",
    "3. The position/order within the barrage (via positional encoding)\n",
    "\n",
    "This way, if most trajectories in a barrage look like Label 1, a borderline trajectory will be \"pulled\" towards Label 1 by the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fa87f",
   "metadata": {},
   "source": [
    "## Transformer Input Explanation\n",
    "\n",
    "### What is the Input to the Transformer?\n",
    "\n",
    "The input is a **sequence of trajectory feature vectors**, where:\n",
    "\n",
    "**Each trajectory is represented as a 49-dimensional feature vector** containing:\n",
    "\n",
    "**Scalar aggregated features** (from `extract_trajectory_features`):\n",
    "- Position stats: `x`, `y`, `z` (first, last, min, max, mean, std) - 18 features\n",
    "- Time info: `total_seconds` (first, last, count) - 3 features  \n",
    "- Velocity stats: `speed` (first, mean, max, std), `v_x/v_y/v_z` (first, mean, std) - 13 features\n",
    "- Path: `step_dist_sum`, `duration`, `dist_start_end`, `straightness` - 4 features\n",
    "- Launch angles: `launch_azimuth`, `launch_elevation` - 2 features\n",
    "\n",
    "**Physics features** (from `extract_physics_features`):\n",
    "- Parabolic fit coefficients: `cx_2, cx_1, cx_0`, etc. for x, y, z - 9 features\n",
    "- Fit residuals: `res_x, res_y, res_z` - 3 features\n",
    "- Acceleration estimates: `acc_x, acc_y, acc_z`, `v0_fit_x/y/z` - 6 features\n",
    "\n",
    "### Barrage Grouping\n",
    "\n",
    "Trajectories are grouped into **barrages** - sequences of trajectories from the same launch location within a 5-minute time window.\n",
    "\n",
    "### Transformer Input Shape\n",
    "\n",
    "`(batch_size, seq_len, 49)` where:\n",
    "- `batch_size` = number of barrages in the batch\n",
    "- `seq_len` = number of trajectories in each barrage (variable, padded)\n",
    "- `49` = feature dimension per trajectory\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Input: [traj_1_features, traj_2_features, ..., traj_N_features]  # N trajectories in barrage\n",
    "   |\n",
    "Input Projection (49 -> 128)\n",
    "   |\n",
    "Positional Encoding (preserves temporal order)\n",
    "   |\n",
    "Self-Attention (each trajectory attends to ALL others in barrage)\n",
    "   |\n",
    "Classification Head (128 -> 3 classes per trajectory)\n",
    "```\n",
    "\n",
    "The key insight is that **self-attention allows each trajectory's prediction to be informed by all other trajectories in the same barrage**, capturing the \"whole picture\" context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7791f",
   "metadata": {},
   "source": [
    "## Feature Dictionary\n",
    "\n",
    "### Scalar Aggregated Features (from `extract_trajectory_features`)\n",
    "\n",
    "| Feature | Meaning |\n",
    "|---------|---------|\n",
    "| `x_first` | X coordinate at trajectory start |\n",
    "| `x_last` | X coordinate at trajectory end |\n",
    "| `x_min` | Minimum X coordinate along trajectory |\n",
    "| `x_max` | Maximum X coordinate along trajectory |\n",
    "| `x_mean` | Mean X coordinate along trajectory |\n",
    "| `x_std` | Standard deviation of X coordinates |\n",
    "| `y_first` | Y coordinate at trajectory start |\n",
    "| `y_last` | Y coordinate at trajectory end |\n",
    "| `y_min` | Minimum Y coordinate along trajectory |\n",
    "| `y_max` | Maximum Y coordinate along trajectory |\n",
    "| `y_mean` | Mean Y coordinate along trajectory |\n",
    "| `y_std` | Standard deviation of Y coordinates |\n",
    "| `z_first` | Z coordinate (altitude) at trajectory start |\n",
    "| `z_last` | Z coordinate (altitude) at trajectory end |\n",
    "| `z_min` | Minimum altitude along trajectory |\n",
    "| `z_max` | Maximum altitude (apex) along trajectory |\n",
    "| `z_mean` | Mean altitude along trajectory |\n",
    "| `z_std` | Standard deviation of altitude |\n",
    "| `total_seconds_first` | Timestamp of first point (nanoseconds) |\n",
    "| `total_seconds_last` | Timestamp of last point (nanoseconds) |\n",
    "| `total_seconds_count` | Number of points in trajectory |\n",
    "| `speed_first` | Speed at trajectory start |\n",
    "| `speed_mean` | Average speed along trajectory |\n",
    "| `speed_max` | Maximum speed along trajectory |\n",
    "| `speed_std` | Standard deviation of speed |\n",
    "| `v_x_first` | X-velocity at trajectory start |\n",
    "| `v_x_mean` | Mean X-velocity along trajectory |\n",
    "| `v_x_std` | Standard deviation of X-velocity |\n",
    "| `v_y_first` | Y-velocity at trajectory start |\n",
    "| `v_y_mean` | Mean Y-velocity along trajectory |\n",
    "| `v_y_std` | Standard deviation of Y-velocity |\n",
    "| `v_z_first` | Z-velocity (vertical) at trajectory start |\n",
    "| `v_z_mean` | Mean Z-velocity along trajectory |\n",
    "| `v_z_std` | Standard deviation of Z-velocity |\n",
    "| `step_dist_sum` | Total path length (sum of all step distances) |\n",
    "| `duration` | Total trajectory duration (nanoseconds) |\n",
    "| `dist_start_end` | Euclidean distance from start to end point |\n",
    "| `straightness` | `dist_start_end / step_dist_sum` (1.0 = perfectly straight) |\n",
    "| `launch_azimuth` | Horizontal launch angle: `atan2(v_y_first, v_x_first)` (radians) |\n",
    "| `launch_elevation` | Vertical launch angle: `atan2(v_z_first, sqrt(v_x^2 + v_y^2))` (radians) |\n",
    "\n",
    "### Physics Features (from `extract_physics_features`)\n",
    "\n",
    "**Parabolic Fit Coefficients**: Each axis is fit to `position(t) = c_2*t^2 + c_1*t + c_0`\n",
    "\n",
    "| Feature | Meaning |\n",
    "|---------|---------|\n",
    "| `cx_2` | X parabolic coefficient (t^2 term) - related to X acceleration |\n",
    "| `cx_1` | X linear coefficient (t term) - initial X velocity from fit |\n",
    "| `cx_0` | X constant term - initial X position from fit |\n",
    "| `cy_2` | Y parabolic coefficient (t^2 term) - related to Y acceleration |\n",
    "| `cy_1` | Y linear coefficient (t term) - initial Y velocity from fit |\n",
    "| `cy_0` | Y constant term - initial Y position from fit |\n",
    "| `cz_2` | Z parabolic coefficient (t^2 term) - should be ~-4.9 for gravity (m/s^2 / 2) |\n",
    "| `cz_1` | Z linear coefficient (t term) - initial vertical velocity from fit |\n",
    "| `cz_0` | Z constant term - initial altitude from fit |\n",
    "| `res_x` | RMSE residual of X parabolic fit (how well it fits) |\n",
    "| `res_y` | RMSE residual of Y parabolic fit |\n",
    "| `res_z` | RMSE residual of Z parabolic fit |\n",
    "\n",
    "**Velocity Linear Fit**: Velocity is fit to `v(t) = acc*t + v0_fit`\n",
    "\n",
    "| Feature | Meaning |\n",
    "|---------|---------|\n",
    "| `acc_x` | X acceleration (slope of v_x over time) |\n",
    "| `acc_y` | Y acceleration (slope of v_y over time) |\n",
    "| `acc_z` | Z acceleration (slope of v_z over time) - should be ~-9.8 m/s^2 for gravity |\n",
    "| `v0_fit_x` | Fitted initial X velocity (intercept) |\n",
    "| `v0_fit_y` | Fitted initial Y velocity (intercept) |\n",
    "| `v0_fit_z` | Fitted initial Z velocity (intercept) |\n",
    "\n",
    "### Physical Interpretation\n",
    "\n",
    "- **`cz_2`**: For ideal ballistic motion under gravity, `z(t) = z0 + v0*t - 0.5*g*t^2`, so `cz_2 = -0.5*g  -4.9`\n",
    "- **`acc_z`**: Should be approximately `-9.8 m/s^2` for objects in free fall\n",
    "- **`straightness`**: Values close to 1.0 indicate straight trajectories; lower values indicate curved paths\n",
    "- **Residuals (`res_x/y/z`)**: Low values indicate the trajectory follows a parabolic path well (ballistic motion); high values may indicate non-ballistic behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
